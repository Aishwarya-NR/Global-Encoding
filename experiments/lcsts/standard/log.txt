pretrain:	
emb_size:	512
learning_rate:	0.0003
dropout:	0.0
shared_vocab:	True
restore:	
schedule:	False
mode:	train
module:	seq2seq
seed:	1234
data:	/home/linjunyang/data/lcsts/data/
logF:	experiments/lcsts/
pool_size:	0
refF:	
length_norm:	True
dec_num_layers:	2
save_interval:	3000
tgt_vocab_size:	4001
optim:	adam
eval_interval:	10000
num_processes:	4
src_vocab_size:	4005
metrics:	['rouge']
unk:	True
model:	seq2seq
max_split:	0
enc_num_layers:	2
log:	standard
epoch:	20
max_time_step:	50
schesamp:	False
split_num:	0
batch_size:	64
swish:	True
attention:	luong_gate
cell:	lstm
hops:	1
config:	lcsts.yaml
use_cuda:	True
gpus:	[0]
hidden_size:	512
selfatt:	True
start_decay_at:	6
char:	False
learning_rate_decay:	0.5
scale:	1
beam_size:	10
bidirectional:	True
max_grad_norm:	10
optim:	adam
length_norm:	True
logF:	experiments/lcsts/
src_vocab_size:	4005
module:	seq2seq
max_grad_norm:	10
enc_num_layers:	2
schedule:	False
swish:	True
tgt_vocab_size:	4001
learning_rate_decay:	0.5
scale:	1
metrics:	['rouge']
use_cuda:	False
model:	seq2seq
max_time_step:	50
pool_size:	0
max_split:	0
refF:	
beam_size:	10
unk:	True
batch_size:	64
data:	/home/linjunyang/data/lcsts/data/
hops:	1
cell:	lstm
num_processes:	4
bidirectional:	True
mode:	train
epoch:	20
shared_vocab:	True
selfatt:	True
eval_interval:	10000
save_interval:	3000
emb_size:	512
dropout:	0.0
start_decay_at:	6
hidden_size:	512
dec_num_layers:	2
attention:	luong_gate
char:	False
schesamp:	False
gpus:	[0]
seed:	1234
pretrain:	
split_num:	0
config:	lcsts.yaml
learning_rate:	0.0003
restore:	
log:	standard
optim:	adam
length_norm:	True
logF:	experiments/lcsts/
src_vocab_size:	4005
module:	seq2seq
max_grad_norm:	10
enc_num_layers:	2
schedule:	False
swish:	True
tgt_vocab_size:	4001
learning_rate_decay:	0.5
scale:	1
metrics:	['rouge']
use_cuda:	False
model:	seq2seq
max_time_step:	50
pool_size:	0
max_split:	0
refF:	
beam_size:	10
unk:	True
batch_size:	64
data:	/home/linjunyang/data/lcsts/data/
hops:	1
cell:	lstm
num_processes:	4
bidirectional:	True
mode:	train
epoch:	20
shared_vocab:	True
selfatt:	True
eval_interval:	10000
save_interval:	3000
emb_size:	512
dropout:	0.0
start_decay_at:	6
hidden_size:	512
dec_num_layers:	2
attention:	luong_gate
char:	False
schesamp:	False
gpus:	[0]
seed:	1234
pretrain:	
split_num:	0
config:	lcsts.yaml
learning_rate:	0.0003
restore:	
log:	standard

seq2seq(
  (encoder): rnn_encoder(
    (embedding): Embedding(4005, 512)
    (dropout): Dropout(p=0.02)
    (sw1): Sequential(
      (0): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True)
      (2): ReLU()
    )
    (sw3): Sequential(
      (0): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
      (1): ReLU()
      (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True)
      (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))
      (4): ReLU()
      (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True)
    )
    (sw33): Sequential(
      (0): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
      (1): ReLU()
      (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True)
      (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))
      (4): ReLU()
      (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True)
      (6): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))
      (7): ReLU()
      (8): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True)
    )
    (swish): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))
    (linear): Sequential(
      (0): Linear(in_features=1024, out_features=1024, bias=True)
      (1): GLU(dim=-1)
      (2): Dropout(p=0.0)
    )
    (filter_linear): Linear(in_features=1536, out_features=512, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (PosEnc): PositionalEncoding(
      (dropout): Dropout(p=0.0)
    )
    (attention): luong_gate_attention(
      (linear_in): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
        (1): Dropout(p=0.1)
      )
      (linear_out): Sequential(
        (0): Linear(in_features=1024, out_features=512, bias=True)
        (1): SELU
        (2): Dropout(p=0.1)
        (3): Linear(in_features=512, out_features=512, bias=True)
        (4): SELU
        (5): Dropout(p=0.1)
      )
      (softmax): Softmax()
      (selu): SELU
    )
    (rnn): LSTM(512, 512, num_layers=2, bidirectional=True)
  )
  (decoder): rnn_decoder(
    (embedding): Embedding(4005, 512)
    (rnn): StackedLSTM(
      (dropout): Dropout(p=0.0)
      (layers): ModuleList(
        (0): LSTMCell(512, 512)
        (1): LSTMCell(512, 512)
      )
    )
    (linear): Linear(in_features=512, out_features=4001, bias=True)
    (linear_): Linear(in_features=512, out_features=512, bias=True)
    (sigmoid): Sigmoid()
    (attention): luong_gate_attention(
      (linear_in): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
        (1): Dropout(p=0.1)
      )
      (linear_out): Sequential(
        (0): Linear(in_features=1024, out_features=512, bias=True)
        (1): SELU
        (2): Dropout(p=0.1)
        (3): Linear(in_features=512, out_features=512, bias=True)
        (4): SELU
        (5): Dropout(p=0.1)
      )
      (softmax): Softmax()
      (selu): SELU
    )
    (dropout): Dropout(p=0.0)
  )
  (log_softmax): LogSoftmax()
  (criterion): CrossEntropyLoss(
  )
)

total number of parameters: 27473313

selfatt:	True
src_vocab_size:	4005
pool_size:	0
optim:	adam
refF:	
schesamp:	False
unk:	True
hops:	1
save_interval:	3000
num_processes:	4
attention:	luong_gate
dec_num_layers:	2
eval_interval:	10000
batch_size:	64
logF:	experiments/lcsts/
learning_rate:	0.0003
max_split:	0
data:	/home/linjunyang/data/lcsts/data/
length_norm:	True
seed:	1234
config:	lcsts.yaml
emb_size:	512
max_time_step:	50
beam_size:	10
model:	seq2seq
use_cuda:	False
learning_rate_decay:	0.5
log:	standard
mode:	train
bidirectional:	True
gpus:	[0]
metrics:	['rouge']
scale:	1
shared_vocab:	True
epoch:	20
split_num:	0
hidden_size:	512
restore:	
pretrain:	
module:	seq2seq
schedule:	False
tgt_vocab_size:	4001
cell:	lstm
swish:	True
enc_num_layers:	2
max_grad_norm:	10
start_decay_at:	6
char:	False
dropout:	0.0
selfatt:	True
src_vocab_size:	4005
pool_size:	0
optim:	adam
refF:	
schesamp:	False
unk:	True
hops:	1
save_interval:	3000
num_processes:	4
attention:	luong_gate
dec_num_layers:	2
eval_interval:	10000
batch_size:	64
logF:	experiments/lcsts/
learning_rate:	0.0003
max_split:	0
data:	/home/linjunyang/data/lcsts/data/
length_norm:	True
seed:	1234
config:	lcsts.yaml
emb_size:	512
max_time_step:	50
beam_size:	10
model:	seq2seq
use_cuda:	False
learning_rate_decay:	0.5
log:	standard
mode:	train
bidirectional:	True
gpus:	[0]
metrics:	['rouge']
scale:	1
shared_vocab:	True
epoch:	20
split_num:	0
hidden_size:	512
restore:	
pretrain:	
module:	seq2seq
schedule:	False
tgt_vocab_size:	4001
cell:	lstm
swish:	True
enc_num_layers:	2
max_grad_norm:	10
start_decay_at:	6
char:	False
dropout:	0.0

seq2seq(
  (encoder): rnn_encoder(
    (embedding): Embedding(4005, 512)
    (dropout): Dropout(p=0.02)
    (sw1): Sequential(
      (0): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True)
      (2): ReLU()
    )
    (sw3): Sequential(
      (0): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
      (1): ReLU()
      (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True)
      (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))
      (4): ReLU()
      (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True)
    )
    (sw33): Sequential(
      (0): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
      (1): ReLU()
      (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True)
      (3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))
      (4): ReLU()
      (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True)
      (6): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))
      (7): ReLU()
      (8): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True)
    )
    (swish): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))
    (linear): Sequential(
      (0): Linear(in_features=1024, out_features=1024, bias=True)
      (1): GLU(dim=-1)
      (2): Dropout(p=0.0)
    )
    (filter_linear): Linear(in_features=1536, out_features=512, bias=True)
    (tanh): Tanh()
    (sigmoid): Sigmoid()
    (PosEnc): PositionalEncoding(
      (dropout): Dropout(p=0.0)
    )
    (attention): luong_gate_attention(
      (linear_in): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
        (1): Dropout(p=0.0)
      )
      (linear_out): Sequential(
        (0): Linear(in_features=1024, out_features=512, bias=True)
        (1): SELU
        (2): Dropout(p=0.0)
        (3): Linear(in_features=512, out_features=512, bias=True)
        (4): SELU
        (5): Dropout(p=0.0)
      )
      (softmax): Softmax()
      (selu): SELU
    )
    (rnn): LSTM(512, 512, num_layers=2, bidirectional=True)
  )
  (decoder): rnn_decoder(
    (embedding): Embedding(4005, 512)
    (rnn): StackedLSTM(
      (dropout): Dropout(p=0.0)
      (layers): ModuleList(
        (0): LSTMCell(512, 512)
        (1): LSTMCell(512, 512)
      )
    )
    (linear): Linear(in_features=512, out_features=4001, bias=True)
    (linear_): Linear(in_features=512, out_features=512, bias=True)
    (sigmoid): Sigmoid()
    (attention): luong_gate_attention(
      (linear_in): Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
        (1): Dropout(p=0.0)
      )
      (linear_out): Sequential(
        (0): Linear(in_features=1024, out_features=512, bias=True)
        (1): SELU
        (2): Dropout(p=0.0)
        (3): Linear(in_features=512, out_features=512, bias=True)
        (4): SELU
        (5): Dropout(p=0.0)
      )
      (softmax): Softmax()
      (selu): SELU
    )
    (dropout): Dropout(p=0.0)
  )
  (log_softmax): LogSoftmax()
  (criterion): CrossEntropyLoss(
  )
)

total number of parameters: 27473313

